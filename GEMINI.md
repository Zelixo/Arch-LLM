# Project Context: Arch-LLM

**Arch-LLM** is a native Linux desktop application built with Rust and GTK4 that serves as a client for [Ollama](https://ollama.com/), allowing users to interact with local Large Language Models (LLMs). It is designed with Arch Linux in mind, including a `PKGBUILD` for easy system integration.

## ðŸ“‚ Project Structure

### Key Files
*   **`src/main.rs`**: The core application logic. This single file handles:
    *   UI construction (GTK4 Application Window, Widgets, CSS styling).
    *   State management (Application state, chat history, settings).
    *   Ollama integration (Client setup, streaming chat requests).
    *   Markdown rendering (converting Markdown to Pango markup for GTK labels).
*   **`Cargo.toml`**: Rust project configuration and dependencies.
    *   *Key Dependencies:* `gtk4`, `ollama-rs`, `tokio`, `serde`, `pulldown-cmark`.
*   **`PKGBUILD`**: Arch Linux package build script used to create an installable `.zst` package.
*   **`settings.json`**: Persistent storage for application settings.
    *   Stores: Ollama API endpoint, defined Agents (system prompts), and User Profiles.
*   **`history.json`**: Persistent storage for past chat sessions.

## ðŸš€ Building and Running

### Prerequisites
Ensure you have the following installed:
*   Rust & Cargo
*   GTK4 development libraries
*   Ollama (running locally, usually on port 11434)

### Development
To run the application directly during development:

```bash
cargo run
```

### Packaging for Arch Linux
To build an installable package using the `PKGBUILD`:

```bash
makepkg -si
```

## ðŸ› ï¸ Configuration & Features

### Settings (`settings.json`)
The application attempts to connect to a local Ollama instance (default: `http://localhost:11434`). This can be configured in the UI or directly in `settings.json`.

**Features:**
*   **Startup:** Robust connection check with Retry logic if Ollama is unreachable.
*   **Agents:** Users can define multiple "Agents" with specific models (e.g., `llama3`, `gemma:2b`) and custom system prompts (personas).
*   **Profiles:** Users can create profiles with personal details (Bio, Location, etc.) to provide context to the LLM.
*   **Chat History:** Sessions are saved automatically. Titles are auto-generated by the LLM after the first few messages.
*   **Chat UX:**
    *   Multi-line input (Enter to send, Shift+Enter for newline).
    *   Auto-scrolling during generation.
    *   "Stop" button to abort long responses.
    *   **Thinking Spinner** for immediate feedback.
    *   **Copy Button** for one-click response copying.
    *   Markdown rendering with syntax highlighting support for code blocks (using styled TextViews).
    *   **Context Menu:** Right-click history items to Rename or Delete chats.
    *   **Shortcuts:**
        *   `Ctrl+N`: New Chat
        *   `Ctrl+,`: Settings
        *   `Ctrl+Q`: Quit
*   **Model Management:** Settings tab to list installed models and pull new ones from Ollama.

## ðŸ’» Development Conventions

*   **UI Construction:** The UI is built programmatically in Rust (within `src/main.rs`) rather than using external `.ui` XML files.
*   **Styling:** CSS is embedded directly in the Rust code (`provider.load_from_data(...)`).
*   **Async/Await:**
    *   `tokio` is used for the async runtime and heavy lifting (Ollama requests).
    *   `async_channel` is used to communicate between Tokio threads and the GTK Main Context.
    *   `glib::spawn_local` handles UI updates on the main thread.
*   **Modules:**
    *   `src/main.rs`: UI logic and event handling.
    *   `src/state.rs`: Data structures (`AppState`, `Settings`, `ChatHistory`).
    *   `src/utils.rs`: Helper functions (`parse_markdown`, `normalize_url`).

## âš ï¸ Notes
*   **Configuration & Data:** The application adheres to XDG standards:
    *   **Settings:** `~/.config/arch-llm/settings.json`
    *   **History:** `~/.local/share/arch-llm/history.json`
*   Markdown support is implemented by parsing Markdown events and converting them to Pango markup tags (e.g., `**text**` -> `<b>text</b>`).
